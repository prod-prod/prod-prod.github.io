---
title: 'Simple pipeline with python, SQL and Crontab'
date: 2021-12-13
permalink: /posts/2021/12/simple-pipeline/
tags:
  - pipeline
  - crontab
  - python
---

Pipeline is one of the core element of the data engineering. Of course there are multiple powerful tools to build advanced pipelines. In the article I will show how you can build a simple pipeline with python3 and SQL only.

## What is a data pipeline

**Data pipeline** - a series of steps in which data is processed.

**Data Validation** is the process of ensuring that data is present, correct & meaningful. Ensuring the quality of your data through automated validation checks is a critical step in building data pipelines at any organization.

**Directed Acyclic Graphs (DAGs)**: DAGs are a special subset of graphs in which the edges between nodes have a specific direction, and no cycles exist. When we say “no cycles exist” what we mean is the nodes cant create a path back to themselves.

**Nodes**: A step in the data pipeline process.

**Edges**: The dependencies or relationships other between nodes.

## Simple pipeline flow 

Our pipeline copies data from production database (emulated in Mysql) to target database (Postgres). For simplicity and easy reproduction we use Docker containers to run databases.

In our case we will implement the following scenarios.
Scenario to start databases:
* Start Docker containers with Mysql and Postgres databases
* Create tables needed for our scenario
* Emulate production database data generation

Scenario of data migration:
* Connect to Postgres and Mysql databases with python (2 classes)
* Select missing data from production database 
* Insert selected data in the another database in batches
* Repeat previous steps

## Implementation

You can find code in [Git repo](https://github.com/prod-prod/simple-pipeline).

Quick code walk through.

Mysql and Psql classes-modules have been created for repeated use of the functions open_conn, run_query, get_result, close_conn.

> IMPORTANT: Keep in mind that the goal of the article not to show the best python implementation but just to show the use case. So don't expect the high quality code and don't use it in production.  

Here is the implementation of the Mysql class.
```python
import mysql.connector


class Mysql:
    def __init__(self):
        self.cursor = None
        self.conn = None

    def open_conn(self, db_name, host_name, port_num, user_name, passw=None):
        self.conn = mysql.connector.connect(
            user=user_name,
            password=passw,
            host=host_name,
            database=db_name,
            port=port_num
        )

    def run_query(self, run_query, to_commit):
        if self.cursor is not None:
            self.cursor.close()

        if to_commit:
            self.cursor = self.conn.cursor()
            self.cursor.execute(run_query)
            self.conn.commit()
            self.cursor.close()
            self.cursor = None
            return 0
        else:
            self.cursor = self.conn.cursor(buffered=True)
            self.cursor.execute(run_query)
            return self.cursor.rowcount

    def get_result(self, count):
        if self.cursor is None or count <= 0:
            return []

        result = list()
        while count > 0:
            row = self.cursor.fetchone()

            if row is None:
                self.cursor.close()
                self.cursor = None
                break
            else:
                result.append(row)
                count -= 1

        return result

    def close_conn(self):
        if self.cursor is not None:
            self.cursor.close()

        if self.conn is not None:
            self.conn.close()

```

Here is the implementation of the Psql class.
```python
import psycopg2


class Psql:
    def __init__(self):
        self.cursor = None
        self.conn = None

    def open_conn(self, db_name, host_name, port_num, user_name, passw = None):
        # Establishing the connection
        self.conn = psycopg2.connect(
            database=db_name, 
            user=user_name, 
            password=passw, 
            host=host_name, 
            port= port_num
        )

        
    def run_query(self, run_query, to_commit):
        if self.cursor is not None:
            self.cursor.close()

        if to_commit:
            self.cursor = self.conn.cursor()
            self.cursor.execute(run_query)
            self.conn.commit()
            self.cursor.close()
            self.cursor = None

        else:
            self.cursor = self.conn.cursor()
            self.cursor.execute(run_query)
            return self.cursor.rowcount

    
    def get_result(self, count):
        return self.cursor.fetchmany(count)


    def close_conn(self):
        if self.cursor is not None:
            self.cursor.close()

        elif self.conn is not None:
            self.conn.close()  
```
These classes are used in the scripts that create tables (create_tables.py), insert data (insert_data.py) and copy data (copy_data.py).

Create tables script starts at the beginning to make sure all needed tables are available.
```python
# create_tables.py
from modules.my_sql import Mysql
from modules.postgres import Psql


if __name__ == "__main__":
    db_mysql = Mysql()
    db_mysql.open_conn(db_name="mysql", host_name="127.0.0.1",
                       port_num="3306", user_name="root", passw="123456")

    db_mysql.run_query("""
    create table sys.orders
        (order_id text,
        created datetime);
    """, True)

    db_mysql.close_conn()

    db_psql = Psql()
    db_psql.open_conn(db_name="postgres", host_name="127.0.0.1",
                      port_num="5432", user_name="postgres", passw="mypassword")

    db_psql.run_query("""
    create table public.orders
        (order_id text primary key,
        created timestamp);
    """, True)

    db_psql.close_conn()

```

Insert data will emulate a production database with inserting random 10 rows each time we will run the script. A delay of 1 second was introduced to generate slightly different timestamp on each run.
```python
# insert_data.py
from modules.my_sql import Mysql
import uuid
import time


query_template =  "insert into sys.orders values ('{}', current_timestamp());"

if __name__ == "__main__":
    db = Mysql()
    db.open_conn(db_name="mysql", host_name="127.0.0.1", port_num="3306", user_name="root",passw="123456")
    for i in range(10):
        id = str(uuid.uuid1())    
        db.run_query(query_template.format(id), True)
        time.sleep(1)

    db.close_conn()
    
```

Copy data is the most complicated task of the flow. It is basically an implementation of a pipeline where we copy missing data from production to the target database.
```python
# copy_data.py
from modules.my_sql import Mysql
from modules.postgres import Psql

'''
1 connect to postgres - 
2 sel max date from postgres table
3 open connection to mysql
4 select * from sys.orders where date >= max date from postgres
5 get result and insert data in postgres (keep uuid unique)
'''

if __name__=="__main__":
    # open connection to target database
    db_destination = Psql()
    db_destination.open_conn(db_name="postgres", host_name="127.0.0.1", port_num="5432", user_name="postgres",passw="mypassword")
    
    # get last record
    count_match = db_destination.run_query("select max(created) from public.orders", False)   
    max_date = db_destination.get_result(count_match)
    
    extract_query = "select * from sys.orders"

    if max_date[0][0] is not None:
        extract_query += " where created >= '{}'".format(max_date[0][0])
    extract_query += ";"

    # open connection to production database
    db_source = Mysql()
    db_source.open_conn(db_name="mysql", host_name="127.0.0.1", port_num="3306", user_name="root",passw="123456")

    # select missing data
    trans_data = db_source.run_query(extract_query, False)

    # insert missing data in a batch of 10 records
    # the batch size can be changed based on a usecase
    batch_size = 10
    while trans_data > 0:
        rows = db_source.get_result(extract_query, batch_size)
        trans_data -= batch_size

        load_query = "insert into public.orders values "
        
        for id, created in rows:
            load_query += "('{}', '{}'),".format(id, created)    
        load_query = load_query[:len(load_query)-1] + " "
        load_query += "on conflict on constraint orders_pkey do nothing;"    

        db_destination.run_query(load_query, True)
    
    # make sure we close all opened connections
    db_destination.close_conn()
    db_source.close_conn()
```

## How to run

Get the code from Git.
```bash
git clone https://github.com/prod-prod/simple-pipeline.git

cd simple-pipeline/
```

Make sure you have docker installed. Visit [Docker Official Page](https://docs.docker.com/engine/install/ubuntu/).

Install python dependencies:
```bash
python pip3 install -r requirements.txt
```

Start the docker:
```bash
bash start_db.sh
```

Now you can on any iteration run insert data (emulation of the production DB):
```bash
python3 insert_data.py
```

And you can run pipeline:
```bash
python3 copy_data.py
```

## Schedule iterations
You might notice we run scripts only once, but updating a database is something that should be executed regularly. For this purpose used [Crontab](https://man7.org/linux/man-pages/man5/crontab.5.html).

### How to install
Detailed information how to install Cron can be found [here](https://www.geeksforgeeks.org/how-to-setup-cron-jobs-in-ubuntu/).

### How to schedule a task
Scheduling is defined by 5 parameters. For people who never used it before, I highly recommend to visit [https://crontab.guru/](https://crontab.guru/). It really easy to navigate and get your needed frequency.

In our case we want to run insert_data.py every 2 minutes and copy_data.py every 10 minutes.

So the `/etc/crontab` will look like this:

```text
*/2 * * * * python3 insert_data.py
*/10 * * * * python3 copy_data.py
```

That is it!

After that, we need to complete the test and run the following command:

```bash
bash stop_db.sh
```

## Conclusions
So, pure python pipeline implementation has pros and cons.
From my perspective pros are:
* no need to install additional software
* simple infrastructure
* easy to get it up and running

Cons are:
* limited functionality out of the box (like no logging, retry, alarms etc.)
* code can get complicated quickly

Happy coding! 
