---
title: 'Simple pipeline with Airflow'
date: 2022-01-20
permalink: /posts/2022/01/pipeline1-airflow/
tags:
  - pipeline
  - airflow
  - python
---


This post is about the option of replacing Crontab with Airflow. Python code from the [previous post](https://prod-prod.github.io//posts/2021/12/simple-pipeline/) is used in the article, supplemented by DAG (insert_data.py, copy_data.py) and edited bash code (start_db.sh, stop_db.sh).

## Data pipeline frameworks

Here are most commonly used batch workflow schedulers: Airflow, Luigi, Oozie, or Azkaban. The tools allow users to programmatically specify workflows as tasks with dependencies between them, automate and monitor these workflows.

In our case we will use Apache Airflow.   

## Quick introduction to airflow

Apache Airflow is a workflow engine that easily schedules and runs data pipelines. In this platform, each task of the data pipeline will be executed in a certain order, and each task will receive the necessary resources. Airflow uses Python to implement pipelines and define dependencies between tasks.

## How to install and run airflow
Installation of Airflow is possible via multiply ways. The officially supported way to install is by using `pip`. 

If you use Ubuntu OS to install Apache Airflow run the following command:
```bash
pip install "apache-airflow[celery]==2.2.3" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.2.3/constraints-3.6.txt"
```
If you face any problems visit the [official installation page](https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html).

Initialize Airflow Database

Initialize the airflow database from the ***$AIRFLOW_HOME/airflow*** directory using the following commands:
```bash
airflow db init
airflow webserver -p 8080
```
Create the folder where all DAGs will be stored and specify the path to the folder in the ***airflow.cfg*** file with ***dags_folder*** parameter (optional, by default it's ***dags*** folder).

 Create a new user

To create a user with username as admin with Admin role run the following code:

```bash
airflow users create --username admin --password your_password --firstname your_first_name --lastname your_last_name --role Admin --email your_email@some.com
```

## Create DAG and schedule it

Now we are ready to implement our DAG.

The sequence of task execution is specified via DAG (directed acyclic graph). Apache Airflow provides a special DAG object that implements the context manager protocol.

Relationships between tasks are formed using bitwise shift operators. For example, given t3 >> t2 >> t1, task t3 will be executed first, then t2, then t3. Parallel execution in Python code sets like: 
```python
t3 >> t2 >> t1
t3 >> t4 >> t5
```
Means that t2 and t4 will be executed in parallel.


Let's take a look at the implementation of the data insert process:

```python
# file: insert_data.py

from modules.my_sql import Mysql
import uuid
import time
from airflow import DAG
from datetime import timedelta, datetime


query_template =  "insert into sys.orders values ('{}', current_timestamp());"

with DAG(
    dag_id='insert_data',
    schedule_interval=timedelta(minutes=2),
    start_date=datetime(2021,12,21),
    catchup=False,
    dagrun_timeout=timedelta(minutes=20),
    tags=['insert data', 'simple pipeline'],
) as dag:
    db = Mysql()
    db.open_conn(db_name="mysql", host_name="127.0.0.1", port_num="3306", user_name="root",passw="123456")
    for i in range(10):
        id = str(uuid.uuid1())    
        db.run_query(query_template.format(id), True)
        time.sleep(1)

    db.close_conn()
```

The python script contains a DAG with following properties:  
`dag_id` - the mandatory parameter that defines a name of the DAG.  
`schedule_interval` (datetime.timedelta or dateutil.relativedelta.relativedelta or str that acts as a cron expression) – defines how often that DAG runs, this timedelta object gets added to your latest task instance’s execution_date to figure out the next schedule.  
`start_date` (datetime.datetime) – the timestamp from which the scheduler will attempt to backfill.  
`catchup` (bool) – Perform scheduler catchup (or only run latest)? Defaults to True.  
`dagrun_timeout `(datetime.timedelta) – specify how long a DagRun should be up before timing out / failing, so that new DagRuns can be created. The timeout is only enforced for scheduled DagRuns.  
`tags` (List[str]) – list of tags to help filtering DAGs in the UI.


To know more about parameters visit the [official documentation](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html).
 
 Now let's have a look on more complicated script, that implements copying the data from one database and inserting to another:

```python
# file: copy_data.py

from modules.my_sql import Mysql
from modules.postgres import Psql
from airflow import DAG
from datetime import timedelta, datetime


'''
Flow:
1 connect to postgres  
2 get max date from postgres table
3 open connection to mysql
4 select * from sys.orders where date >= max date from postgres
5 get result and insert data in postgres (keep uuid unique)
'''

with DAG(
    dag_id='copy_data',
    schedule_interval=timedelta(minutes=6),
    start_date=datetime(2021,12,21),
    catchup=False,
    dagrun_timeout=timedelta(minutes=30),
    tags=['copy data', 'simple pipeline'],
) as dag:
    db_destination = Psql()
    db_destination.open_conn(db_name="postgres", host_name="127.0.0.1", port_num="5432", user_name="postgres",passw="mypassword")
    
    count_match = db_destination.run_query("select max(created) from public.orders", False)   
    max_date = db_destination.get_result(count_match)
    
    extract_query = "select * from sys.orders"

    if max_date[0][0] is not None:
        extract_query += " where created >= '{}'".format(max_date[0][0])
    extract_query += ";"

    db_source = Mysql()
    db_source.open_conn(db_name="mysql", host_name="127.0.0.1", port_num="3306", user_name="root",passw="123456")

    trans_data = db_source.run_query(extract_query, False)
    batch_size = 5

    while trans_data > 0:
        rows = db_source.get_result(batch_size)
        trans_data -= batch_size
        load_query = "insert into public.orders values "
        
        for id, created in rows:
            load_query += "('{}', '{}'),".format(id, created)    
        load_query = load_query[:len(load_query)-1] + " "
        load_query += "on conflict on constraint orders_pkey do nothing;"


        db_destination.run_query(load_query, True)
    

    db_destination.close_conn()
    db_source.close_conn()
```

To make the file visible to Airflow, it is copied to the ***dags_folder*** specified when you installed Airflow.

As we can see from our example comparing to use-case with Crontab, Airflow doesn't add too much complexity to python code. But Airflow provides many benefits comparing to the Crontab, such as splitting tasks in multiple steps, run steps in parallel, monitor errors and states. 

## CLI Airflow

Many users use Web-interface working with Airflow, it's convenient and simple. But there is another way to manage tasks and schedule DAGs with command line. I recommend to get familiar with main Airflow commands: 

```bash
airflow cheatsheet
```
If you want to get more info visit official [reference](https://airflow.apache.org/docs/apache-airflow/1.10.9/cli.html).


## Conclusion

Pros:
- Airflow is user-friendly and convenient tool to build pipelines. 
- Big community, a lot of documentation, how-to, troubleshooting available
- A lot of useful features like dependencies manager, scheduler, webserver

Cons:
- hard to setup for beginners