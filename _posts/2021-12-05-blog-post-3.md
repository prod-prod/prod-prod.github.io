---
title: 'Simple pipeline with python, SQL and Airflow'
date: 2022-01-20
permalink: /posts/2022/01/pipeline1-airflow/
tags:
  - pipeline
  - airflow
  - python
---


This post is about the option of replacing Crontab with Airflow. In the article used python code from the [previous post](https://prod-prod.github.io//posts/2021/12/simple-pipeline/), supplemented by DAG (insert_data.py, copy_data.py) and edited bash code (start_db.sh, stop_db.sh).

## Data pipeline frameworks

Here are most commonly used batch workflow schedulers: Airflow, Luigi, Oozie, or Azkaban. The tools allow users to programmatically specify workflows as tasks with dependencies between them, automate and monitor these workflows.

In our case we will use Apache Airflow.   

## Quick introduction to airflow

Apache Airflow is a workflow engine that easily schedules and runs data pipelines. In this platform, each task of the data pipeline will be executed in a certain order, and each task will receive the necessary resources. Airflow uses Python to implement pipelines and define dependencies between tasks.

## How to install and run airflow
If you use Ubuntu OS to install Apache Airflow follow next steps:
1. Upgrade OS and install pip
```bash
sudo apt update
sudo apt install python3-pip
```
Verify the installation by checking the pip version:
```bash
pip3 --version
```
2. Install Airflow Dependencies

To make sure the necessary dependencies are installed you need to run the following commands:
```bash
sudo apt-get install libmysqlclient-dev
sudo apt-get install libssl-dev
sudo apt-get install libkrb5-dev
```
3. Install virtual environment

Airflow installation will be done in the Python environment, so we have to install Python environment first. So, run the following commands:
```bash
sudo apt install python3-virtualenv
virtualenv airflow_workspace
cd airflow_workspace/bin/
source activate
# Airflow needs a home. `~/airflow` is the default, but you can put it somewhere else if you prefer (optional)
export AIRFLOW_HOME=~/airflow
```
4. Install Apache Airflow

In the ***~/airflow_otodigi/bin*** directory run the following commands:
```bash
install apache-airflow
```
5. Initialize Airflow Database

Initialize the airflow database from the ***~/airflow_workspace/airflow*** directory using the following commands:
```bash
airflow db init
airflow webserver -p 8080
```
Create the folder where all DAGs will be stored and specify the path to the folder in the ***airflow.cfg*** file with ***dags_folder*** parameter (optional, by default it's ***dags*** folder).

6. Create a new user

To create a user with username as admin with Admin role run the following code:

```bash
airflow users create --username admin --password your_password --firstname your_first_name --lastname your_last_name --role Admin --email your_email@some.com
```

## Create DAG and schedule it

The sequence of task execution is specified via DAG (directed acyclic graph). Apache Airflow provides a special DAG object that implements the context manager protocol.

Relationships between tasks are formed using bitwise shift operators. For example, given t3 >> t2 >> t1, task t3 will be executed first, then t2, then t3. Parallel execution in Python code sets like: 
```python
t3 >> t2 >> t1
t3 >> t4 >> t5
```
Means that t2 and t4 will be executed in parallel.


Let's take a look at the implementation of the data insert process:

```python
# file: insert_data.py

from modules.my_sql import Mysql
import uuid
import time
from airflow import DAG
from datetime import timedelta, datetime


query_template =  "insert into sys.orders values ('{}', current_timestamp());"

with DAG(
    dag_id='insert_data',
    schedule_interval=timedelta(minutes=2),
    start_date=datetime(2021,12,21),
    catchup=False,
    dagrun_timeout=timedelta(minutes=20),
    tags=['insert data', 'simple pipeline'],
) as dag:
    db = Mysql()
    db.open_conn(db_name="mysql", host_name="127.0.0.1", port_num="3306", user_name="root",passw="123456")
    for i in range(10):
        id = str(uuid.uuid1())    
        db.run_query(query_template.format(id), True)
        time.sleep(1)

    db.close_conn()
```

The python script contains a DAG with following properties:
dag_id - the mandatory parameter that defines a name of the DAG

```python
# file: copy_data.py

from modules.my_sql import Mysql
from modules.postgres import Psql
from airflow import DAG
from datetime import timedelta, datetime


'''
1 connect to postgres - 
2 sel max date from postgres table
3 open connection to mysql
4 select * from sys.orders where date >= max date from postgres
5 get result and insert data in postgres (keep uuid unique)
'''

with DAG(
    dag_id='copy_data',
    schedule_interval=timedelta(minutes=6),
    start_date=datetime(2021,12,21),
    catchup=False,
    dagrun_timeout=timedelta(minutes=30),
    tags=['copy data', 'simple pipeline'],
) as dag:
    db_destination = Psql()
    db_destination.open_conn(db_name="postgres", host_name="127.0.0.1", port_num="5432", user_name="postgres",passw="mypassword")
    
    count_match = db_destination.run_query("select max(created) from public.orders", False)   
    max_date = db_destination.get_result(count_match)
    
    extract_query = "select * from sys.orders"

    if max_date[0][0] is not None:
        extract_query += " where created >= '{}'".format(max_date[0][0])
    extract_query += ";"

    db_source = Mysql()
    db_source.open_conn(db_name="mysql", host_name="127.0.0.1", port_num="3306", user_name="root",passw="123456")

    trans_data = db_source.run_query(extract_query, False)
    batch_size = 5

    while trans_data > 0:
        rows = db_source.get_result(batch_size)
        trans_data -= batch_size
        load_query = "insert into public.orders values "
        
        for id, created in rows:
            load_query += "('{}', '{}'),".format(id, created)    
        load_query = load_query[:len(load_query)-1] + " "
        load_query += "on conflict on constraint orders_pkey do nothing;"


        db_destination.run_query(load_query, True)
    

    db_destination.close_conn()
    db_source.close_conn()
```

To make the file visible to Airflow, it is copied to the ***dags_folder*** specified when you installed Airflow.


## Conclusion

<!---
[link](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html)


To run the scenario in th Apache Airflow 


[1](https://airflow.apache.org/docs/apache-airflow/stable/start/local.html)



[2](https://progressivecoder.com/airflow-dag-example-create-your-first-dag/)

--->